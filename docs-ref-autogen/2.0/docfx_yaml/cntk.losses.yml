api_name: []
items:
- _type: module
  children: []
  module: cntk.losses
  name: cntk.losses
  summary: ''
  type: Namespace
  uid: cntk.losses
- _type: class
  children:
  - cntk.losses.binary_cross_entropy
  - cntk.losses.cosine_distance
  - cntk.losses.cosine_distance_with_negative_samples
  - cntk.losses.cross_entropy_with_softmax
  - cntk.losses.lambda_rank
  - cntk.losses.squared_error
  - cntk.losses.weighted_binary_cross_entropy
  module: cntk.losses
  name: cntk.losses.Global
  summary: Proxy object to hold module level functions
  type: Class
  uid: cntk.losses.Global
- _type: function
  module: cntk.losses
  name: cntk.losses.binary_cross_entropy
  summary: 'Computes the binary cross entropy (aka logistic loss) between the ``output``
    and ``target``.


    :param output: the computed posterior probability for a variable to be 1 from
    the network (typ. a ``sigmoid``)

    :param target: ground-truth label, 0 or 1

    :param name: the name of the Function instance in the network

    :type name: str, optional


    :returns: :class:`~cntk.ops.functions.Function`


    .. todo:: add an example

    '
  type: Method
  uid: cntk.losses.binary_cross_entropy
- _type: function
  module: cntk.losses
  name: cntk.losses.cosine_distance
  summary: "Computes the cosine distance between ``x`` and ``y``:\n\n.. admonition::\
    \ Example\n\n   >>> a = np.asarray([-1, -1, -1, 1, 1, -1, 1, 1, -1, 1, 1, -1]).reshape(3,2,2)\n\
    \   >>> b = np.asarray([1, 1, -1, 1, 1, -1, 1, -1, -1, -1, -1, 1]).reshape(3,2,2)\n\
    \   >>> x = C.sequence.input_variable(shape=(2,))\n   >>> y = C.sequence.input_variable(shape=(2,))\n\
    \   >>> np.round(C.cosine_distance(x,y).eval({x:a,y:b}),5)\n   array([[-1.,  1.],\n\
    \          [ 1.,  0.],\n          [ 0., -1.]], dtype=float32)\n\n:param x: numpy\
    \ array or any :class:`~cntk.ops.functions.Function` that outputs a tensor\n:param\
    \ name: the name of the Function instance in the network\n:type name: str, optional\n\
    \n:returns: :class:`~cntk.ops.functions.Function`\n"
  type: Method
  uid: cntk.losses.cosine_distance
- _type: function
  module: cntk.losses
  name: cntk.losses.cosine_distance_with_negative_samples
  summary: "Given minibatches for ``x`` and ``y``, this function computes for each\
    \ element in `x` the cosine distance between\nit and the corresponding `y` and\
    \ additionally the cosine distance between ``x`` and some other elements of ``y``\n\
    (referred to a negative samples). The ``x`` and ``y`` pairs are samples often\
    \ derived\nfrom embeddings of textual data, though the function can be used for\
    \ any form of numeric encodings.\nWhen using this function to compute textual\
    \ similarity, ``x`` represents search query term embedding\nand ``y`` represents\
    \ a document embedding. The negative samples are formed on the fly by shifting\n\
    the right side (``y``). The ``shift`` indicates how many samples in ``y`` one\
    \ should shift while\nforming each negative sample pair. It is often chosen to\
    \ be 1. As the name suggests\n``num_negative_samples`` indicates how many negative\
    \ samples one would want to generate.\n\n.. admonition:: Example\n\n   >>> qry\
    \ = np.asarray([1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.], dtype=np.float32).reshape(3,\
    \ 1, 4)\n   >>> doc = np.asarray([1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\
    \ 1.], dtype=np.float32).reshape(3, 1, 4)\n   >>> x = C.sequence.input_variable(shape=(4,))\n\
    \   >>> y = C.sequence.input_variable(shape=(4,))\n   >>> model = C.cosine_distance_with_negative_samples(x,\
    \ y, shift=1, num_negative_samples=2)\n   >>> np.round(model.eval({x: qry, y:\
    \ doc}), decimals=4)\n   array([[[ 1. ,  0.5,  0. ]],\n   <BLANKLINE>\n      \
    \    [[ 1. ,  0.5,  0.5]],\n   <BLANKLINE>\n          [[ 1. ,  0. ,  0.5]]], dtype=float32)\n\
    \n:param x: numpy array or any :class:`~cntk.ops.functions.Function` that outputs\
    \ a tensor\n:param y: numpy array or any :class:`~cntk.ops.functions.Function`\
    \ that outputs a tensor\n:param shift: non-zero positive integer representing\
    \ number of shift to generate a negative sample\n:param num_negative_samples:\
    \ number of negative samples to generate, a non-zero positive integer\n:param\
    \ name: the name of the Function instance in the network\n:type name: str, optional\n\
    \n:returns: :class:`~cntk.ops.functions.Function`\n"
  type: Method
  uid: cntk.losses.cosine_distance_with_negative_samples
- _type: function
  module: cntk.losses
  name: cntk.losses.cross_entropy_with_softmax
  summary: "This operation computes the cross entropy between the ``target_vector``\
    \ and\nthe softmax of the ``output_vector``. The elements of ``target_vector``\n\
    have to be non-negative and should sum to 1. The ``output_vector`` can\ncontain\
    \ any values. The function will internally compute the softmax of\nthe ``output_vector``.\
    \ Concretely,\n\n:math:`\\mathrm{softmax}(x)=\\left[\\frac{\\exp(x_1)}{\\sum_i\\\
    exp(x_i)}\\quad\\frac{\\exp(x_1)}{\\sum_i\\exp(x_i)}\\quad\\ldots\\quad\\frac{\\\
    exp(x_1)}{\\sum_i\\exp(x_i)}\\right]`\n\n:math:`\\mathrm{cross\\_entropy\\_with\\\
    _softmax}(o, t) = -\\sum_{i} t_i \\log(\\mathrm{softmax}(o)_i)`\n\nwith the understanding\
    \ that the implementation can use equivalent formulas\nfor efficiency and numerical\
    \ stability.\n\n.. admonition:: Example\n\n   >>> C.cross_entropy_with_softmax([[1.,\
    \ 1., 1., 50.]], [[0., 0., 0., 1.]]).eval()\n   array([[ 0.]], dtype=float32)\n\
    \   \n   >>> C.cross_entropy_with_softmax([[1., 2., 3., 4.]], [[0.35, 0.15, 0.05,\
    \ 0.45]]).eval()\n   array([[ 1.84019]], dtype=float32)\n\n:param output_vector:\
    \ the unscaled computed output values from the network\n:param target_vector:\
    \ usually it is one-hot vector where the hot bit\n                      corresponds\
    \ to the label index. But it can be any probability\n                      distribution\
    \ over the labels.\n:param axis: if given, cross entropy will be computed\n  \
    \           along this axis\n:type axis: int or :class:`~cntk.axis.Axis`, optional\n\
    :param name: the name of the Function instance in the network\n:type name: str,\
    \ optional\n\n:returns: :class:`~cntk.ops.functions.Function`\n"
  type: Method
  uid: cntk.losses.cross_entropy_with_softmax
- _type: function
  module: cntk.losses
  name: cntk.losses.lambda_rank
  summary: "Groups samples according to ``group``, sorts\nthem within each group based\
    \ on ``output`` and\ncomputes the Normalized Discounted Cumulative Gain\n(NDCG)\
    \ at infinity for each group. Concretely,\nthe Discounted Cumulative Gain (DCG)\
    \ at infinity is:\n\n:math:`\\mathrm{DCG_{\\infty}}()=\\sum_{i=0}^{\\infty} \\\
    frac{gain_{(i)}}{\\log(i+2)}`\n\nwhere :math:`gain_{(i)}` means the gain of the\
    \ :math:`i`-th ranked sample.\n\nThe NDCG is just the DCG  divided by the maximum\
    \ achievable DCG (obtained\nby placing the samples with the largest gain at the\
    \ top of the ranking).\n\nSamples in the same group must appear in order of decreasing\
    \ gain.\n\nIt returns 1 minus the average NDCG across all the groups in the minibatch\n\
    multiplied by 100 times the number of samples in the minibatch.\n\nIn the backward\
    \ direction it back-propagates LambdaRank gradients.\n\n.. admonition:: Example\n\
    \n   >>> group = C.input_variable((1,))\n   >>> score = C.input_variable((1,),\
    \ needs_gradient=True)\n   >>> gain  = C.input_variable((1,))\n   >>> g = np.array([1,\
    \ 1, 2, 2], dtype=np.float32).reshape(4,1)\n   >>> s = np.array([1, 2, 3, 4],\
    \ dtype=np.float32).reshape(4,1)\n   >>> n = np.array([7, 1, 3, 1], dtype=np.float32).reshape(4,1)\n\
    \   >>> f = C.lambda_rank(score, gain, group)\n   >>> np.round(f.grad({score:s,\
    \ gain:n, group: g}, wrt=[score]),4)\n   array([[-0.2121],\n   <BLANKLINE>\n \
    \         [ 0.2121],\n   <BLANKLINE>\n          [-0.1486],\n   <BLANKLINE>\n \
    \         [ 0.1486]], dtype=float32)\n\n:param output: score of each sample\n\
    :param gain: gain of each sample\n:param group: group of each sample\n:param name:\
    \ the name of the Function instance in the network\n:type name: str, optional\n\
    \n:returns: :class:`~cntk.ops.functions.Function`\n"
  type: Method
  uid: cntk.losses.lambda_rank
- _type: function
  module: cntk.losses
  name: cntk.losses.squared_error
  summary: "This operation computes the sum of the squared difference between elements\n\
    in the two input matrices. The result is a scalar (i.e., one by one matrix).\n\
    This is often used as a training criterion.\n\n.. admonition:: Example\n\n   >>>\
    \ i1 = C.input_variable((1,2))\n   >>> i2 = C.input_variable((1,2))\n   >>> C.squared_error(i1,i2).eval({i1:np.asarray([[[2.,\
    \ 1.]]], dtype=np.float32), i2:np.asarray([[[4., 6.]]], dtype=np.float32)})\n\
    \   array([ 29.], dtype=float32)\n   \n   >>> C.squared_error(i1,i2).eval({i1:np.asarray([[[1.,\
    \ 2.]]], dtype=np.float32), i2:np.asarray([[[1., 2.]]], dtype=np.float32)})\n\
    \   array([ 0.], dtype=float32)\n\n:param output: the output values from the network\n\
    :param target: it is usually a one-hot vector where the hot bit\n            \
    \   corresponds to the label index\n:param name: the name of the Function instance\
    \ in the network\n:type name: str, optional\n\n:returns: :class:`~cntk.ops.functions.Function`\n"
  type: Method
  uid: cntk.losses.squared_error
- _type: function
  module: cntk.losses
  name: cntk.losses.weighted_binary_cross_entropy
  summary: 'This operation computes the weighted binary cross entropy (aka logistic
    loss) between the ``output`` and ``target``.


    :param output: the computed posterior probability from the network

    :param target: ground-truth label, 0 or 1

    :param weight: weight of each example

    :param name: the name of the Function instance in the network

    :type name: str, optional


    :returns: :class:`~cntk.ops.functions.Function`


    .. todo:: add an example

    '
  type: Method
  uid: cntk.losses.weighted_binary_cross_entropy
