### YamlMime:UniversalReference
api_name: []
items:
- children:
  - cntk.layers.blocks.ForwardDeclaration
  - cntk.layers.blocks.GRU
  - cntk.layers.blocks.LSTM
  - cntk.layers.blocks.RNNStep
  - cntk.layers.blocks.RNNUnit
  - cntk.layers.blocks.Stabilizer
  - cntk.layers.blocks.UntestedBranchError
  fullName: cntk.layers.blocks
  langs:
  - python
  module: cntk.layers.blocks
  name: blocks
  source:
    id: blocks
    path: bindings/python/cntk\layers\blocks.py
    remote:
      branch: release/2.1
      path: bindings/python/cntk\layers\blocks.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 0
  summary: 'Basic building blocks that are semantically not layers (not used in a
    layered fashion),

    e.g. the LSTM block.

    '
  type: module
  uid: cntk.layers.blocks
- example:
  - "\n```\n\n>>> # create a graph with a recurrent loop to compute the length of\
    \ an input sequence\n>>> from cntk.layers.typing import *\n>>> x = C.input_variable(**Sequence[Tensor[2]])\n\
    >>> ones_like_input = C.sequence.broadcast_as(1, x)  # sequence of scalar ones\
    \ of same length as input\n>>> out_fwd = ForwardDeclaration()  # placeholder for\
    \ the state variables\n>>> out = C.sequence.past_value(out_fwd, initial_state=0)\
    \ + ones_like_input\n>>> out_fwd.resolve_to(out)\n>>> length = C.sequence.last(out)\n\
    >>> x0 = np.reshape(np.arange(6,dtype=np.float32),(1,3,2))\n>>> x0\n    array([[[\
    \ 0.,  1.],\n            [ 2.,  3.],\n            [ 4.,  5.]]], dtype=float32)\n\
    >>> length(x0)\n    array([ 3.], dtype=float32)\n```\n"
  fullName: cntk.layers.blocks.ForwardDeclaration
  langs:
  - python
  module: cntk.layers.blocks
  name: ForwardDeclaration
  source:
    id: ForwardDeclaration
    path: bindings/python/cntk\layers\blocks.py
    remote:
      branch: release/2.1
      path: bindings/python/cntk\layers\blocks.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 83
  summary: 'Helper for recurrent network declarations.

    Returns a placeholder variable with an added method `resolve_to()` to be called

    at the end to close the loop.

    This is used for explicit graph building with recurrent connections.

    '
  syntax:
    content: ForwardDeclaration(name='forward_declaration')
    parameters:
    - defaultValue: forward_declaration
      id: name
    return:
      description: 'a placeholder variable with a method `resolve_to()` that resolves
        it to another variable

        '
      type:
      - cntk.variables.Variable
  type: function
  uid: cntk.layers.blocks.ForwardDeclaration
- example:
  - '

    ```


    >>> # a gated recurrent layer

    >>> from cntk.layers import *

    >>> gru_layer = Recurrence(GRU(500))

    ```

    '
  fullName: cntk.layers.blocks.GRU
  langs:
  - python
  module: cntk.layers.blocks
  name: GRU
  source:
    id: GRU
    path: bindings/python/cntk\layers\blocks.py
    remote:
      branch: release/2.1
      path: bindings/python/cntk\layers\blocks.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 468
  summary: 'Layer factory function to create a GRU block for use inside a recurrence.

    The GRU block implements one step of the recurrence and is stateless. It accepts
    the previous state as its first argument,

    and outputs its new state.

    '
  syntax:
    content: GRU(shape, cell_shape=None, activation=tanh, init=glorot_uniform(), init_bias=0,
      enable_self_stabilization=False, name='')
    parameters:
    - description: 'vector or tensor dimension of the output of this layer

        '
      id: shape
      type:
      - int or tuple of ints
    - defaultValue: None
      description: 'if given, then the output state is first computed at *cell_shape*

        and linearly projected to *shape*

        '
      id: cell_shape
      type:
      - tuple, defaults to None
    - description: 'function to apply at the end, e.g. *relu*

        '
      id: activation
      type:
      - 'cntk.ops.functions.Function

        , defaults to @cntk.ops.tanh'
    - description: 'initial value of weights *W*

        '
      id: init
      type:
      - 'scalar or NumPy array or @cntk.initializer

        , defaults to glorot_uniform'
    - description: 'initial value of weights *b*

        '
      id: init_bias
      type:
      - 'scalar or NumPy array or @cntk.initializer

        , defaults to 0'
    - description: 'if *True* then add a @cntk.layers.blocks.Stabilizer

        to all state-related projections (but not the data input)

        '
      id: enable_self_stabilization
      type:
      - bool, defaults to False
    - defaultValue: ''
      description: 'the name of the Function instance in the network

        '
      id: name
      type:
      - str, defaults to ''
    return:
      description: 'A function `(prev_h, input) -> h` that implements one step of
        a recurrent GRU layer.

        '
      type:
      - cntk.ops.functions.Function
  type: function
  uid: cntk.layers.blocks.GRU
- example:
  - '

    ```


    >>> # a typical recurrent LSTM layer

    >>> from cntk.layers import *

    >>> lstm_layer = Recurrence(LSTM(500))

    ```

    '
  fullName: cntk.layers.blocks.LSTM
  langs:
  - python
  module: cntk.layers.blocks
  name: LSTM
  source:
    id: LSTM
    path: bindings/python/cntk\layers\blocks.py
    remote:
      branch: release/2.1
      path: bindings/python/cntk\layers\blocks.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 356
  summary: 'Layer factory function to create an LSTM block for use inside a recurrence.

    The LSTM block implements one step of the recurrence and is stateless. It accepts
    the previous state as its first two arguments,

    and outputs its new state as a two-valued tuple `(h,c)`.

    '
  syntax:
    content: LSTM(shape, cell_shape=None, activation=tanh, use_peepholes=False, init=glorot_uniform(),
      init_bias=0, enable_self_stabilization=False, name='')
    parameters:
    - description: 'vector or tensor dimension of the output of this layer

        '
      id: shape
      type:
      - int or tuple of ints
    - defaultValue: None
      description: 'if given, then the output state is first computed at *cell_shape*

        and linearly projected to *shape*

        '
      id: cell_shape
      type:
      - tuple, defaults to None
    - description: 'function to apply at the end, e.g. *relu*

        '
      id: activation
      type:
      - 'cntk.ops.functions.Function

        , defaults to @cntk.ops.tanh'
    - description: ''
      id: use_peepholes
      type:
      - bool, defaults to False
    - description: 'initial value of weights *W*

        '
      id: init
      type:
      - 'scalar or NumPy array or @cntk.initializer

        , defaults to glorot_uniform'
    - description: 'initial value of weights *b*

        '
      id: init_bias
      type:
      - 'scalar or NumPy array or @cntk.initializer

        , defaults to 0'
    - description: 'if *True* then add a @cntk.layers.blocks.Stabilizer

        to all state-related projections (but not the data input)

        '
      id: enable_self_stabilization
      type:
      - bool, defaults to False
    - defaultValue: ''
      description: 'the name of the Function instance in the network

        '
      id: name
      type:
      - str, defaults to ''
    return:
      description: 'A function `(prev_h, prev_c, input) -> (h, c)` that implements
        one step of a recurrent LSTM layer.

        '
      type:
      - cntk.ops.functions.Function
  type: function
  uid: cntk.layers.blocks.LSTM
- example:
  - '

    ```


    >>> # a plain relu RNN layer

    >>> from cntk.layers import *

    >>> relu_rnn_layer = Recurrence(RNNStep(500, activation=C.relu))

    ```

    '
  fullName: cntk.layers.blocks.RNNStep
  langs:
  - python
  module: cntk.layers.blocks
  name: RNNStep
  source:
    id: RNNStep
    path: bindings/python/cntk\layers\blocks.py
    remote:
      branch: release/2.1
      path: bindings/python/cntk\layers\blocks.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 400
  summary: 'Layer factory function to create a plain RNN block for use inside a recurrence.

    The RNN block implements one step of the recurrence and is stateless. It accepts
    the previous state as its first argument,

    and outputs its new state.

    '
  syntax:
    content: RNNStep(shape, cell_shape=None, activation=sigmoid, init=glorot_uniform(),
      init_bias=0, enable_self_stabilization=False, name='')
    parameters:
    - description: 'vector or tensor dimension of the output of this layer

        '
      id: shape
      type:
      - int or tuple of ints
    - defaultValue: None
      description: 'if given, then the output state is first computed at *cell_shape*

        and linearly projected to *shape*

        '
      id: cell_shape
      type:
      - tuple, defaults to None
    - description: 'function to apply at the end, e.g. *relu*

        '
      id: activation
      type:
      - 'cntk.ops.functions.Function

        , defaults to signmoid'
    - description: 'initial value of weights *W*

        '
      id: init
      type:
      - 'scalar or NumPy array or @cntk.initializer

        , defaults to glorot_uniform'
    - description: 'initial value of weights *b*

        '
      id: init_bias
      type:
      - 'scalar or NumPy array or @cntk.initializer

        , defaults to 0'
    - description: 'if *True* then add a @cntk.layers.blocks.Stabilizer

        to all state-related projections (but not the data input)

        '
      id: enable_self_stabilization
      type:
      - bool, defaults to False
    - defaultValue: ''
      description: 'the name of the Function instance in the network

        '
      id: name
      type:
      - str, defaults to ''
    return:
      description: 'A function `(prev_h, input) -> h` where `h = activation(input
        @ W + prev_h @ R + b)`

        '
      type:
      - cntk.ops.functions.Function
  type: function
  uid: cntk.layers.blocks.RNNStep
- fullName: cntk.layers.blocks.RNNUnit
  langs:
  - python
  module: cntk.layers.blocks
  name: RNNUnit
  source:
    id: RNNUnit
    path: bindings/python/cntk\layers\blocks.py
    remote:
      branch: release/2.1
      path: bindings/python/cntk\layers\blocks.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 444
  summary: 'This is a deprecated name for @cntk.layers.blocks.RNNStep. Use that name
    instead.

    '
  syntax:
    content: RNNUnit(shape, cell_shape=None, activation=sigmoid, init=glorot_uniform(),
      init_bias=0, enable_self_stabilization=False, name='')
    parameters:
    - id: shape
    - defaultValue: None
      id: cell_shape
    - id: activation
    - id: init
    - id: init_bias
    - id: enable_self_stabilization
    - defaultValue: ''
      id: name
  type: function
  uid: cntk.layers.blocks.RNNUnit
- example:
  - '

    ```


    >>> # recurrent model with self-stabilization

    >>> from cntk.layers import *

    >>> with default_options(enable_self_stabilization=True): # enable stabilizers
    by default for LSTM()

    ...     model = Sequential([

    ...         Embedding(300),

    ...         Stabilizer(),           # stabilizer for main data input of recurrence

    ...         Recurrence(LSTM(512)),  # LSTM owns its own stabilizers for the recurrent
    connections

    ...         Stabilizer(),

    ...         Dense(10)

    ...     ])

    ```

    '
  fullName: cntk.layers.blocks.Stabilizer
  langs:
  - python
  module: cntk.layers.blocks
  name: Stabilizer
  source:
    id: Stabilizer
    path: bindings/python/cntk\layers\blocks.py
    remote:
      branch: release/2.1
      path: bindings/python/cntk\layers\blocks.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 137
  summary: "Layer factory function to create a [Droppo self-stabilizer](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/SelfLR.pdf).\n\
    It multiplies its input with a scalar that is learned.\n\nThis takes *enable_self_stabilization*\
    \ as a flag that allows to disable itself. Useful if this is a global default.\n\
    \nNote: Some other layers (specifically, recurrent units like @cntk.layers.blocks.LSTM)\
    \ also have the option to use the `Stabilizer()` layer internally. That is enabled\
    \ by passing *enable_self_stabilization=True* to those layers. In conjunction\
    \ with those, the rule is that an explicit `Stabilizer()` must be inserted by\
    \ the user for the main data input, whereas the recurrent layer will own the stabilizer(s)\
    \ for the internal recurrent connection(s). \n\nNote: Unlike the original paper,\
    \ which proposed a linear or exponential scalar, CNTK uses a sharpened Softplus:\
    \ 1/steepness ln(1+e^{steepness*beta}). The softplus behaves linear for weights\
    \ around and above 1 (like the linear scalar) while guaranteeing positiveness\
    \ (like the exponentional variant) but is also more robust by avoiding exploding\
    \ gradients. \n"
  syntax:
    content: Stabilizer(steepness=4, enable_self_stabilization=True, name='')
    parameters:
    - defaultValue: '4'
      description: ''
      id: steepness
      type:
      - int, defaults to 4
    - description: 'a flag that allows to disable itself. Useful if this is a global
        default

        '
      id: enable_self_stabilization
      type:
      - bool, defaults to False
    - defaultValue: ''
      description: 'the name of the Function instance in the network

        '
      id: name
      type:
      - str, defaults to ''
    return:
      description: 'A function

        '
      type:
      - cntk.ops.functions.Function
  type: function
  uid: cntk.layers.blocks.Stabilizer
- fullName: cntk.layers.blocks.UntestedBranchError
  langs:
  - python
  module: cntk.layers.blocks
  name: UntestedBranchError
  source:
    id: UntestedBranchError
    path: bindings/python/cntk\layers\blocks.py
    remote:
      branch: release/2.1
      path: bindings/python/cntk\layers\blocks.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 28
  syntax:
    content: UntestedBranchError(name)
    parameters:
    - id: name
  type: function
  uid: cntk.layers.blocks.UntestedBranchError
references:
- fullName: cntk.layers.blocks.ForwardDeclaration
  isExternal: false
  name: ForwardDeclaration
  parent: cntk.layers.blocks
  uid: cntk.layers.blocks.ForwardDeclaration
- fullName: cntk.layers.blocks.GRU
  isExternal: false
  name: GRU
  parent: cntk.layers.blocks
  uid: cntk.layers.blocks.GRU
- fullName: cntk.layers.blocks.LSTM
  isExternal: false
  name: LSTM
  parent: cntk.layers.blocks
  uid: cntk.layers.blocks.LSTM
- fullName: cntk.layers.blocks.RNNStep
  isExternal: false
  name: RNNStep
  parent: cntk.layers.blocks
  uid: cntk.layers.blocks.RNNStep
- fullName: cntk.layers.blocks.RNNUnit
  isExternal: false
  name: RNNUnit
  parent: cntk.layers.blocks
  uid: cntk.layers.blocks.RNNUnit
- fullName: cntk.layers.blocks.Stabilizer
  isExternal: false
  name: Stabilizer
  parent: cntk.layers.blocks
  uid: cntk.layers.blocks.Stabilizer
- fullName: cntk.layers.blocks.UntestedBranchError
  isExternal: false
  name: UntestedBranchError
  parent: cntk.layers.blocks
  uid: cntk.layers.blocks.UntestedBranchError
