### YamlMime:UniversalReference
api_name: []
items:
- children:
  - cntk.train.distributed.Communicator
  - cntk.train.distributed.DistributedLearner
  - cntk.train.distributed.WorkerDescriptor
  - cntk.train.distributed.block_momentum_distributed_learner
  - cntk.train.distributed.data_parallel_distributed_learner
  - cntk.train.distributed.mpi_communicator
  fullName: cntk.train.distributed
  langs:
  - python
  module: cntk.train.distributed
  name: distributed
  source:
    id: distributed
    path: bindings/python/cntk\train\distributed.py
    remote:
      branch: release/2.1
      path: bindings/python/cntk\train\distributed.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 0
  summary: 'Distributed learners manage learners in distributed environment.

    '
  type: module
  uid: cntk.train.distributed
- fullName: cntk.train.distributed.block_momentum_distributed_learner
  langs:
  - python
  module: cntk.train.distributed
  name: block_momentum_distributed_learner
  seealsoContent: "See also: [1] K. Chen and Q. Huo. [Scalable training of deep learning\
    \ machines by incremental block training with intra-block parallel optimization\
    \ and blockwise model-update filtering](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/0005880.pdf).\
    \ Proceedings of ICASSP, 2016. \n"
  source:
    id: block_momentum_distributed_learner
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.1
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 148
  summary: 'Creates a block momentum distributed learner. See [1] for more

    information.


    Block Momentum divides the full dataset into M non-overlapping blocks,

    and each block is partitioned into N non-overlapping splits.


    During training, a random, unprocessed block is randomly taken by the trainer

    and the N partitions of this block are dispatched on the workers.

    '
  syntax:
    content: block_momentum_distributed_learner(learner, block_size, block_momentum_as_time_constant=None,
      use_nestrov_momentum=True, reset_sgd_momentum_after_aggregation=True, block_learning_rate=1.0,
      distributed_after=0)
    parameters:
    - description: 'a local learner (i.e. sgd)

        '
      id: learner
    - description: 'size of the partition in samples

        '
      id: block_size
      type:
      - int
    - description: 'block momentum as time constant

        '
      id: block_momentum_as_time_constant
      type:
      - float
    - description: 'use nestrov momentum

        '
      id: use_nestrov_momentum
      type:
      - bool
    - description: 'reset SGD momentum after aggregation

        '
      id: reset_sgd_momentum_after_aggregation
      type:
      - bool
    - description: 'block learning rate

        '
      id: block_learning_rate
      type:
      - float
    - description: 'number of samples after which distributed training starts

        '
      id: distributed_after
      type:
      - int
    return:
      description: 'a distributed learner instance

        '
  type: function
  uid: cntk.train.distributed.block_momentum_distributed_learner
- fullName: cntk.train.distributed.data_parallel_distributed_learner
  langs:
  - python
  module: cntk.train.distributed
  name: data_parallel_distributed_learner
  source:
    id: data_parallel_distributed_learner
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.1
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 122
  summary: 'Creates a data parallel distributed learner

    '
  syntax:
    content: data_parallel_distributed_learner(learner, distributed_after=0, num_quantization_bits=32,
      use_async_buffered_parameter_update=False)
    parameters:
    - description: 'a local learner (i.e. sgd)

        '
      id: learner
    - description: 'number of samples after which distributed training starts

        '
      id: distributed_after
      type:
      - int
    - description: 'number of bits for quantization (1 to 32)

        '
      id: num_quantization_bits
      type:
      - int
    - description: 'use async buffered parameter update

        '
      id: use_async_buffered_parameter_update
      type:
      - bool
    return:
      description: 'a distributed learner instance

        '
  type: function
  uid: cntk.train.distributed.data_parallel_distributed_learner
- fullName: cntk.train.distributed.mpi_communicator
  langs:
  - python
  module: cntk.train.distributed
  name: mpi_communicator
  source:
    id: mpi_communicator
    path: bindings/python/cntk\internal\swig_helper.py
    remote:
      branch: release/2.1
      path: bindings/python/cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 199
  summary: 'Creates a non quantized MPI communicator.

    '
  syntax:
    content: mpi_communicator()
  type: function
  uid: cntk.train.distributed.mpi_communicator
references:
- fullName: cntk.train.distributed.Communicator
  isExternal: false
  name: Communicator
  parent: cntk.train.distributed
  uid: cntk.train.distributed.Communicator
- fullName: cntk.train.distributed.DistributedLearner
  isExternal: false
  name: DistributedLearner
  parent: cntk.train.distributed
  uid: cntk.train.distributed.DistributedLearner
- fullName: cntk.train.distributed.WorkerDescriptor
  isExternal: false
  name: WorkerDescriptor
  parent: cntk.train.distributed
  uid: cntk.train.distributed.WorkerDescriptor
- fullName: cntk.train.distributed.block_momentum_distributed_learner
  isExternal: false
  name: block_momentum_distributed_learner
  parent: cntk.train.distributed
  uid: cntk.train.distributed.block_momentum_distributed_learner
- fullName: cntk.train.distributed.data_parallel_distributed_learner
  isExternal: false
  name: data_parallel_distributed_learner
  parent: cntk.train.distributed
  uid: cntk.train.distributed.data_parallel_distributed_learner
- fullName: cntk.train.distributed.mpi_communicator
  isExternal: false
  name: mpi_communicator
  parent: cntk.train.distributed
  uid: cntk.train.distributed.mpi_communicator
